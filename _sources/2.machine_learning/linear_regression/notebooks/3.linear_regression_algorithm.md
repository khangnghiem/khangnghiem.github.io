## Linear Regression

### Linear Equation

- Linear Equation: $y = $m($x) + $b + (error)
b: bias, m: weight

### Multivariate Linear Regression

- Multivariate Linear Regression: $$y = a_1(x_1) + a_2(x_2) + ... + a_n(x_n) + b + (error)$$

### Linear Regression Algorithm

- Linear Regression Algorithm: a repeating step in which we move a line closer to a data point until the regression line is closer to all the data points
- Use Square Trick or Absolute Trick to reformulate the model by a learning rate. There's also a Simple Trick but this is only useful for learning purposes.
$$\hat{p} = m'x + b'$$
- Square Trick: $x(p-\hat{p})$ will give us a sense to direction to move close to a point (by looking at the quadrants)
with $m' = m + \eta x(p-\hat{p})$ and $b' = b + \eta(p-\hat{p})$

- Absolute Trick:
with $m' = m + \eta x$ and $b' = b + \eta$ if $p > \hat{p}$
else $m' = m - \eta x$ and $b' = b - \eta$

### Learning Rate

- Learning rate is denoted by $\eta$ or 'eta'
- This number helps us make sure our model changes in very small amounts by training. (a random number used to modify / train our model)

### Square Trick Pseudocode

```python
import random
def linear_regression(features, labels, learning_rate=0.01, epochs = 1000):
    price_per_room = random.random()
    base_price = random.random()
    for epoch in range(epochs):
        i = random.randint(0, len(features)-1)
        num_rooms = features[i]
        price = labels[i]
        price_per_room, base_price = square_trick(base_price,
                                                  price_per_room,
                                                  num_rooms,
                                                  price,
                                                  learning_rate=learning_rate)
    return price_per_room, base_price
```

### General Square Trick Pseudocode (Multivariate)

$$\hat{y} = w'_1x_1 + w'_2x_2 + ... + w'_nx_n + b'$$

Very similar to single variable Square Trick except:

- Add $\eta(y-\hat{y})$ to the y-intercept $b$. Obtain $b' = b + \eta(y-\hat{y})$
- For i = 1, 2, 3, ..., n:
  - Add $\eta x(y-\hat{y})$ to the weight $w_i$. Obtain $w'_i = w_i + \eta x(y-\hat{y})$

## Error Function

- The square error is used more commonly in practice than the absolute error.
- Because A square has a much nicer derivative than an absolute value, which comes in handy during the training process.
