## Gradient Descent

- Gradient is a vector denoted as: $\nabla f = (\frac{\delta f}{\delta x_1}, \frac{\delta f}{\delta x_2}, ..., \frac{\delta f}{\delta x_n})$

- $\nabla f > 0$: increasing; $\nabla f < 0$: decreasing

## Gradient Descent Algorithm

- With N: epochs (i.e. iterations) and $\eta$: learning rate
- Pick a random point $p_0$.
- For i = 0, …, N – 1:
– Calculate the gradient $\nabla f(p_i)$.
– Pick the point $p_{i+1} = p_i – \eta\nabla f(p_i)$.
- End with the point $p_n$.

## Batch Gradient Descent Algorithm

- Pick random weights $w_1, w_2, …, w_n$ and a random bias $b$.
- For i = 0, …, N – 1:
  - For each of the mini-batches $B_1, B_2, …, B_l$.
    - Calculate the error function $f(w, b)$ on that particular mini-batch.
    - Calculate the gradient $\nabla f(w_1, w_2, …, w_n, b) = (\frac{\delta f}{\delta w_1}, ..., \frac{\delta f}{\delta w_n}, \frac{\delta f}{\delta b})$
    - Replace the weights and bias as follows:
      - w1 gets replaced by 
      - b gets replaced by 
