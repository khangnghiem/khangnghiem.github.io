## Error Function

- The square error is used more commonly in practice than the absolute error.
- Because A square has a much nicer derivative than an absolute value, which comes in handy during the training process.

## Mean Absolute Error (MAE)

### Formula

Gradient MAE Vector: $\nabla{MAE} = (\frac{\delta{MAE}}{\delta w_i}, ..., \frac{\delta{MAE}}{\delta w_n}, \frac{\delta{MAE}}{\delta b})$

We have: $\frac{\delta{\hat{y_i}}}{w_j} = \sum_{j=1}^{n} \frac{\delta{(w_j x_j^{i} + b)}}{\delta w_j} = \sum_{j=1}^{n} \frac{\delta{(w_j x_j^{i})}}{\delta w_j} = x_j^{(i)}$ &nbsp;&nbsp;&nbsp; (1)

To calculate these derivatives:
$\frac{\delta{MAE}}{\delta w_j} = \frac{1}{q} \sum_{i=1}^q \frac{\delta |\hat{y_i} - y_i| }{\delta{w_j}}$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$= \frac{1}{q} \sum_{i=1}^q sgn(\hat{y_i} - y_i) \frac{\delta\hat{y_i}}{\delta{w_j}}$
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$= \frac{1}{q} \sum_{i=1}^q sgn(\hat{y_i} - y_i) x_j^{(i)}$  &nbsp;&nbsp; from (1)
with $q$ being the number of batches

Derivative of MAE with respect to weights:
$$\frac{\delta{MAE}}{\delta w_j} = \frac{1}{q} \sum_{i=1}^q sgn(\hat{y_i} - y_i) x_j^{(i)}$$

Similarly, with respect to $b$:
$$\frac{\delta{MAE}}{\delta b} = \frac{1}{q} \sum_{i=1}^q sgn(\hat{y_i} - y_i)$$

### Gradient Descent Step

$$w_j' = w_j + \frac{1}{q} \sum_{i=1}^q \eta sgn(y_i - \hat{y_i}) x_j^{(i)}$$
$$b' = b + \eta \sum_{i=1}^q sgn(y_i - \hat{y_i})$$

**NOTE**:
If the mini-batch has size $q = 1$, the formula will become the ***simple trick***

$$w_j' = w_j + \eta sgn(y_i - \hat{y_i}) x_j^{(i)}$$
$$b' = b + \eta sgn(y_i - \hat{y_i})$$

## Root Mean Square Error

```py
def rmse(labels, predictions):
    n = len(labels)
    differences = np.subtract(labels, predictions)
    return np.sqrt(1.0/n * (np.dot(differences, differences)))
```
