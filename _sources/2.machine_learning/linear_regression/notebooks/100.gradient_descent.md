## Gradient Descent

- Gradient is a vector denoted as: $\nabla f = (\frac{\delta f}{\delta x_1}, \frac{\delta f}{\delta x_2}, ..., \frac{\delta f}{\delta x_n})$

- $\nabla f > 0$: increasing; $\nabla f < 0$: decreasing

## Gradient Descent Algorithm

- With N: epochs (i.e. iterations) and $\eta$: learning rate
- Pick a random point $p_0$.
- For i = 0, …, N – 1:
– Calculate the gradient $\nabla f(p_i)$.
– Pick the point $p_{i+1} = p_i – \eta\nabla f(p_i)$.
- End with the point $p_n$.
