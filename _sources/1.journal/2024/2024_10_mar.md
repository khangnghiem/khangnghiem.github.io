# March 2024

## Mar 17, 2024 -

-

## Mar 16, 2024 - Linear Regression in action

- Datasets should be split into three sets: training, validation and testing set
- Coding practice: Move the line ŷ = 2x + 3 closer to the point (x, y) = (5, 15) using the the absolute trick and square trick with learning_rate = 0.01.

```py
def absolute_trick(w, b, features, labels, learning_rate):
    predicted = b + w * features
    if labels > predicted:
        w += learning_rate * features
        b += learning_rate
    else:
        w -= learning_rate * features
        b -= learning_rate
    return w, b

def square_trick(w, b, features, labels, learning_rate):
    predicted = b + w * features
    w += learning_rate * features * (labels - predicted)
    b += learning_rate * (labels - predicted)
    return w, b
```

*Results*

```py
absolute_trick(w=2, b=3, features=5, labels=15, learning_rate=0.01)
# (2.05, 3.01)
square_trick(w=2, b=3, features=5, labels=15, learning_rate=0.01)
# (2.1, 3.02)
```

## Mar 15, 2024 - Polynomial Regression

- Used when datasets are non-linear!
- We must decide the degree of the polynomial function before the training process to determine the necessary columns.
- E.g. Is the line of best fit a line (degree 1), a parabola (degree 2), a cubic (degree 3), or some kind of curve?

## Mar 14, 2024 - Muay Thai First Impression

- Practicing Muay Thai made Khang realize there are so much to learn in Martial Arts.
- Added Elbows to his list of combos
- There is less bouncing in Muay Thai compared to boxing due to the constant kick check.
- Machine Learning: Never train test datasets!!!

## Mar 13, 2024 - Matrix, Derivatives and Integrals

- Khang couldn't believe he has now touched upon these Linear Algebra concepts again after so many years!
- Vector Dot Product is very helpful when calculating mean square:
  - np.dot((1, 2, 3), (1, 2, 3)) = $1\cdot{1} + 2\cdot{2} + 3\cdot{3} = 1^2 + 2^2 + 3^2$

## Mar 12, 2024 - Gradient Descent

- Gradient is a vector denoted as: $\nabla f = (\frac{\delta f}{\delta x_1}, \frac{\delta f}{\delta x_2}, ..., \frac{\delta f}{\delta x_n})$
- Two types of Gradient Descent: Stochastic (one data point at a time) vs Batch (a dataset)
- Gradient Descent Algorithm: refer to {doc}`../../2.machine_learning/linear_regression/notebooks/99.gradient_descent.md`
  - With N: epochs (i.e. iterations) and $\eta$: learning rate
  - Pick a random point $p_0$.
  - For i = 0, …, N – 1:
    – Calculate the gradient $\nabla f(p_i)$.
    – Pick the point $p_{i+1} = p_i – \eta\nabla f(p_i)$.
  - End with the point $p_n$.
- Very useful in decreasing the error function

## Mar 11, 2024 - Linear Regression Algorithm

- Square Trick & Absolute Trick with Square Trick being more popular
- Error Function (aka. loss/cost function): absolute & square errors. However, mean absolute errors (MAE) and mean square errors or root mean square errors (RMSE) are more commonly used in practice because mean errors are easier to compare between models.
- Gradient descent: decrease an error function by slowly descending from a mountain
- BEWARE: of getting stuck in a valley (aka. Getting stuck on local minima) while descending

## Mar 10, 2024 - Linear Regression

- Khang realized it is important for him to start with the basics first: Linear Regression
- Multivariate Linear Regression: $$y = a_1(x_1) + a_2(x_2) + ... + a_n(x_n) + b + (error)$$
- Linear Regression algorithm: a repeating step in which we move a line closer to a data point until the regression line is closer to all the data points
- LaTeX can be embedded in Markdown

## Mar 09, 2024 - Willpower

- Today Khang learned that lying on his bed totally kills his willpower. The reason being willpower is managed by his frontal lobe region which is weakened when his body is fatigue (which is triggered simply lying on his bed in a sleep position)
- Also his screen time needs to be managed as well.

## Mar 08, 2024 - Linear Equation

- Feels like Khang is back to University learning Linear Algebra again...
- Linear Equation: $$y = m(x) + b$$
b: bias, m: weight

## Mar 07, 2024 - Foundation of Machine Learning

- Machine Learning, like Human Learning, begins with Remember (Data), Formulate (Model), Predict
- Khang thinks he should get started with supervised learning

## Mar 06, 2024 - Vacation

- Vacation

## Mar 05, 2024 - Rejections

- Last day before Vacation
- All of his interview results have been rejected

## Mar 04, 2024

- Job Search continues

## Mar 03, 2024 - Kingsmith Walking Pad

- Khang placed an order for Kingsmith Walking Pad so he can walk while working from home.
- He intended to use it to practice his kickboxing skills as well.

## Mar 02, 2024 - Machine Learning Interviews

- Started reading 'Machine Learning Interviews' by Susan Shu Chang
- Susan is an entrepreneur, game developer and a principal data scientist, starting her career with a degree in Economics.

## Mar 01, 2024 - Kickboxing footwork

- Just another day to practice footwork and punching, in mixing order.
- His posture got better, it is important to curl his body and keept his chin tucked in.
