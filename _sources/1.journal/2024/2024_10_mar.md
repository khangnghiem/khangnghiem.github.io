# March 2024

## Mar 20, 2024 - Data Pipeline

```{mermaid}
graph BT
  ETL["ETL Layer"]
  Metadata[("
    Metadata Layer
    E.g. Data source, OLAP, 
    ETL, Warehouse,
    Reposting, Data Mining 
    metadata
  ")]

  subgraph Source["Data Source Layer"]
    Internal
    External
  end
  subgraph ETL["ETL Layer"]
    direction LR
    Extraction -->
    Transformation -->
    Loading
  end
  subgraph WarehouseLayer["Data Warehouse Layer"]
    direction LR
    ODS["Operational Data 
    Store (ODS)
    "] -->
    Warehouse["Data Warehouse"]
    Warehouse --ETL--> Mart1["Data Mart"] --> Front1["BI/BU-facing\nData Warehouse"]
    Warehouse --ETL--> Mart2["Data Mart"] --> Front2["BI/BU-facing\nData Warehouse"]
    Warehouse --ETL--> Mart3["Data Mart"] --> Front3["BI/BU-facing\nData Warehouse"]
  end
  subgraph EndUser["End User Layer"]
    direction BT
    Query["Query and Reporting"] -->
    OLAP["OLAP, Data Mining"] -->
    Visual["Data Visualization"] -->
    Analytical["
      Analytical Apps
      e.g. modelling, forecasting
    "]
  end
  Source-->ETL
  ETL --detailed near\nreal-time data--> ODS
  ETL --Summarised data\nHistorical Data--> Warehouse
  WarehouseLayer --> EndUser
  ETL --> Metadata
  WarehouseLayer --> Metadata
  EndUser --> Metadata
  
```

## Mar 19, 2024 - Airflow Docker Container

- After a bit of tinkering, Khang was able to run airflow in Docker on NAB work device.
- Scheduler can use intervals that are:
  - CRON based:
    - \* \* \* \* \*: (minute) (hour) (day) (month) (day of the week)
    - E.g. 45 23 \* \* SAT = 23:45 every Saturday
  - Presets (run at midnight):
    - @once, @hourly, @daily, etc.
  - Frequency based using python datetime library:
    - E.g. dt.timedelta(days=3)
- DAG's Jinja templating syntax: \{\{variable_name\}\}
  - E.g. "curl -o /data/events/\{\{ds\}\}.json"

## Mar 18, 2024 - Data Pipeline with Airflow

- Started learning Airflow
- First impression:
  - Not very hard to use
  - However, installing in work machine is a pain, both dev (Docker) and local env
  - Easy to visualize DAGs with operator/callable graphs
- 3 core components, these are deployed in separate containers:
  - The webserver
  - The scheduler
  - The worker processes

## Mar 17, 2024 - Complexity Regularization

- Regularize model complexity with L1 and L2 norms
- Named after Henri Lebesgue
  - L1 norm: The sum of the absolute values of the coefficients, or
  - L2 norm: The sum of the squares of the coefficients
  - bias not included since it is unrelated to complexity
- Reducing the cost function with measures of Quality and Complexity: $$Error = Regression\ error + \lambda\ Regularization\ term$$
  - L1 Lasso Regression (Least Absolute Shrinkage and Selection Operator).
    - Often used to reduce the number of features.
  - L2 Ridge Regression (The name comes from the shape of the function).
    - Often used to reduce the value of the coefficients of features.
  - $\lambda$ is the regularization parameter. Bigger $\lambda$ results in simple model.
  - Started learning Flask (Python microframework for backend)

## Mar 16, 2024 - Linear Regression in action

- Datasets should be split into three sets: training, validation and testing set
- Coding practice: Move the line ŷ = 2x + 3 closer to the point (x, y) = (5, 15) using the the absolute trick and square trick with learning_rate = 0.01.

```py
def absolute_trick(w, b, features, labels, learning_rate):
    predicted = b + w * features
    if labels > predicted:
        w += learning_rate * features
        b += learning_rate
    else:
        w -= learning_rate * features
        b -= learning_rate
    return w, b

def square_trick(w, b, features, labels, learning_rate):
    predicted = b + w * features
    w += learning_rate * features * (labels - predicted)
    b += learning_rate * (labels - predicted)
    return w, b
```

*Results*

```py
absolute_trick(w=2, b=3, features=5, labels=15, learning_rate=0.01)
# (2.05, 3.01)
square_trick(w=2, b=3, features=5, labels=15, learning_rate=0.01)
# (2.1, 3.02)
```

## Mar 15, 2024 - Polynomial Regression

- Used when datasets are non-linear!
- We must decide the degree of the polynomial function before the training process to determine the necessary columns.
- E.g. Is the line of best fit a line (degree 1), a parabola (degree 2), a cubic (degree 3), or some kind of curve?

## Mar 14, 2024 - Muay Thai First Impression

- Practicing Muay Thai made Khang realize there are so much to learn in Martial Arts.
- Added Elbows to his list of combos
- There is less bouncing in Muay Thai compared to boxing due to the constant kick check.
- Machine Learning: Never train test datasets!!!

## Mar 13, 2024 - Matrix, Derivatives and Integrals

- Khang couldn't believe he has now touched upon these Linear Algebra concepts again after so many years!
- Vector Dot Product is very helpful when calculating mean square:
  - np.dot((1, 2, 3), (1, 2, 3)) = $1\cdot{1} + 2\cdot{2} + 3\cdot{3} = 1^2 + 2^2 + 3^2$

## Mar 12, 2024 - Gradient Descent

- Gradient is a vector denoted as: $\nabla f = (\frac{\delta f}{\delta x_1}, \frac{\delta f}{\delta x_2}, ..., \frac{\delta f}{\delta x_n})$
- Two types of Gradient Descent: Stochastic (one data point at a time) vs Batch (a dataset)
- Gradient Descent Algorithm: refer to {doc}`../../2.machine_learning/linear_regression/notebooks/99.gradient_descent.md`
  - With N: epochs (i.e. iterations) and $\eta$: learning rate
  - Pick a random point $p_0$.
  - For i = 0, …, N – 1:
    – Calculate the gradient $\nabla f(p_i)$.
    – Pick the point $p_{i+1} = p_i – \eta\nabla f(p_i)$.
  - End with the point $p_n$.
- Very useful in decreasing the error function

## Mar 11, 2024 - Linear Regression Algorithm

- Square Trick & Absolute Trick with Square Trick being more popular
- Error Function (aka. loss/cost function): absolute & square errors. However, mean absolute errors (MAE) and mean square errors or root mean square errors (RMSE) are more commonly used in practice because mean errors are easier to compare between models.
- Gradient descent: decrease an error function by slowly descending from a mountain
- BEWARE: of getting stuck in a valley (aka. Getting stuck on local minima) while descending

## Mar 10, 2024 - Linear Regression

- Khang realized it is important for him to start with the basics first: Linear Regression
- Multivariate Linear Regression: $$y = a_1(x_1) + a_2(x_2) + ... + a_n(x_n) + b + (error)$$
- Linear Regression algorithm: a repeating step in which we move a line closer to a data point until the regression line is closer to all the data points
- LaTeX can be embedded in Markdown

## Mar 09, 2024 - Willpower

- Today Khang learned that lying on his bed totally kills his willpower. The reason being willpower is managed by his frontal lobe region which is weakened when his body is fatigue (which is triggered simply lying on his bed in a sleep position)
- Also his screen time needs to be managed as well.

## Mar 08, 2024 - Linear Equation

- Feels like Khang is back to University learning Linear Algebra again...
- Linear Equation: $$y = m(x) + b$$
b: bias, m: weight

## Mar 07, 2024 - Foundation of Machine Learning

- Machine Learning, like Human Learning, begins with Remember (Data), Formulate (Model), Predict
- Khang thinks he should get started with supervised learning

## Mar 06, 2024 - Vacation

- Vacation

## Mar 05, 2024 - Rejections

- Last day before Vacation
- All of his interview results have been rejected

## Mar 04, 2024

- Job Search continues

## Mar 03, 2024 - Kingsmith Walking Pad

- Khang placed an order for Kingsmith Walking Pad so he can walk while working from home.
- He intended to use it to practice his kickboxing skills as well.

## Mar 02, 2024 - Machine Learning Interviews

- Started reading 'Machine Learning Interviews' by Susan Shu Chang
- Susan is an entrepreneur, game developer and a principal data scientist, starting her career with a degree in Economics.

## Mar 01, 2024 - Kickboxing footwork

- Just another day to practice footwork and punching, in mixing order.
- His posture got better, it is important to curl his body and keept his chin tucked in.
